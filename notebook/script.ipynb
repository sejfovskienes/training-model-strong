{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516b2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82365e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gld_data(live_mode=False, force_refresh=False):\n",
    "    \"\"\"Fetch GLD (Gold ETF) historical data with proper validation and numpy storage\"\"\"\n",
    "    cache_file = 'gld_data_1h.npy'  # Different cache for 1h data\n",
    "    cache_dates_file = 'gld_dates_1h.npy'\n",
    "    \n",
    "    print(\"=== GLD DATA FETCHING DEBUG ===\")\n",
    "    print(f\"Cache files: {cache_file}, {cache_dates_file}\")\n",
    "    print(f\"Cache exists: {os.path.exists(cache_file) and os.path.exists(cache_dates_file)}\")\n",
    "    \n",
    "    # Always fetch fresh data to avoid corruption issues\n",
    "    force_refresh = True  # Force fresh data until we verify integrity\n",
    "    \n",
    "    if not force_refresh and os.path.exists(cache_file) and os.path.exists(cache_dates_file):\n",
    "        try:\n",
    "            cache_time = os.path.getmtime(cache_file)\n",
    "            current_time = time.time()\n",
    "            cache_age = current_time - cache_time\n",
    "            print(f\"Cache age: {cache_age:.0f} seconds ({cache_age/3600:.2f} hours)\")\n",
    "            \n",
    "            # If cache is less than 1 hour old, use it\n",
    "            if cache_age < 3600:  # 3600 seconds = 1 hour\n",
    "                print(\"Loading cached data...\")\n",
    "                data_array = np.load(cache_file)\n",
    "                dates_array = np.load(cache_dates_file, allow_pickle=True)  # Fix: allow pickle for string arrays\n",
    "                \n",
    "                # Convert back to DataFrame\n",
    "                dates = pd.to_datetime(dates_array)\n",
    "                cached_data = pd.DataFrame(data_array, \n",
    "                                         columns=['Open', 'High', 'Low', 'Close', 'Volume'],\n",
    "                                         index=dates)\n",
    "                \n",
    "                print(f\"✓ Using cached data: {len(cached_data)} rows\")\n",
    "                print(f\"✓ Cache date range: {cached_data.index[0]} to {cached_data.index[-1]}\")\n",
    "                print(f\"✓ Cache price range: ${cached_data['Close'].min():.2f} - ${cached_data['Close'].max():.2f}\")\n",
    "                \n",
    "                # Validate current price against real market\n",
    "                current_gld = yf.Ticker('GLD')\n",
    "                current_price = current_gld.info.get('regularMarketPrice', 0)\n",
    "                cached_latest = cached_data['Close'].iloc[-1]\n",
    "                price_diff_pct = abs((current_price - cached_latest) / current_price) * 100\n",
    "                \n",
    "                if price_diff_pct > 10:  # If cached price differs by more than 10%\n",
    "                    print(f\"✗ Cache validation failed: cached ${cached_latest:.2f} vs current ${current_price:.2f} ({price_diff_pct:.1f}% diff)\")\n",
    "                    force_refresh = True\n",
    "                else:\n",
    "                    print(f\"✓ Cache validation passed: price difference {price_diff_pct:.1f}%\")\n",
    "                    return cached_data\n",
    "            else:\n",
    "                print(\"Cache is too old, fetching fresh data...\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Cache load error: {e}, fetching fresh data...\")\n",
    "    \n",
    "    if force_refresh or not os.path.exists(cache_file):\n",
    "        print(\"Fetching fresh data or cache validation failed...\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n=== FETCHING FRESH GLD DATA ===\")\n",
    "        print(\"Connecting to Yahoo Finance for GLD...\")\n",
    "        \n",
    "        # OPTIMIZED: Use 1-hour intervals for extended dataset (Better for GLD analysis)\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=1095)  # 3 years of 1h data = ~26k samples\n",
    "        \n",
    "        print(f\"Requesting 1-HOUR GLD data from {start_date.date()} to {end_date.date()} (3 years)...\")\n",
    "        print(\"Calling yf.download() with 1h interval for GLD (optimal for gold trading)...\")\n",
    "        \n",
    "        # Download 1-hour data with error handling (no 60-day limit)\n",
    "        import time as time_module\n",
    "        fetch_start = time_module.time()\n",
    "        gld = yf.download('GLD', start=start_date, end=end_date, interval='1h', progress=False)\n",
    "        fetch_duration = time_module.time() - fetch_start\n",
    "        \n",
    "        print(f\"Download completed in {fetch_duration:.2f} seconds\")\n",
    "        print(f\"Downloaded data type: {type(gld)}\")\n",
    "        print(f\"Downloaded data empty: {gld.empty if hasattr(gld, 'empty') else 'No empty attr'}\")\n",
    "        \n",
    "        if gld.empty:\n",
    "            raise Exception(\"Downloaded GLD data is empty\")\n",
    "        \n",
    "        print(f\"✓ Raw GLD data shape: {gld.shape}\")\n",
    "        print(f\"✓ Raw columns: {gld.columns.tolist()}\")\n",
    "        print(f\"✓ Raw index type: {type(gld.index)}\")\n",
    "        print(f\"✓ First few dates: {gld.index[:3].tolist()}\")\n",
    "        \n",
    "        # Handle multi-level columns from yfinance\n",
    "        if isinstance(gld.columns, pd.MultiIndex):\n",
    "            print(\"Flattening multi-level columns...\")\n",
    "            gld.columns = [col[0] if col[1] == 'GLD' else f\"{col[0]}_{col[1]}\" for col in gld.columns]\n",
    "        \n",
    "        print(f\"✓ Processed columns: {gld.columns.tolist()}\")\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        missing_cols = [col for col in required_cols if col not in gld.columns]\n",
    "        if missing_cols:\n",
    "            raise Exception(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        print(f\"✓ All required columns present: {required_cols}\")\n",
    "        \n",
    "        # Validate data quality for 1h data (3 years)\n",
    "        if len(gld) < 5000:  # Need substantial data for 1h intervals\n",
    "            raise Exception(f\"Insufficient GLD data: only {len(gld)} rows. Need at least 5000 for 1h training.\")\n",
    "        \n",
    "        nan_count = gld['Close'].isna().sum()\n",
    "        if nan_count > len(gld) * 0.1:\n",
    "            raise Exception(f\"Too many missing Close prices: {nan_count} out of {len(gld)}\")\n",
    "        \n",
    "        print(f\"✓ Data quality check passed: {len(gld)} rows, {nan_count} NaN values\")\n",
    "        \n",
    "        # Remove any rows with NaN in critical columns\n",
    "        original_len = len(gld)\n",
    "        gld = gld.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "        dropped_rows = original_len - len(gld)\n",
    "        \n",
    "        print(f\"✓ Cleaned GLD data shape: {gld.shape} (dropped {dropped_rows} rows)\")\n",
    "        print(f\"✓ Date range: {gld.index[0]} to {gld.index[-1]}\")\n",
    "        print(f\"✓ Price range: ${gld['Close'].min():.2f} - ${gld['Close'].max():.2f}\")\n",
    "        print(f\"✓ Sample recent prices: {gld['Close'].tail(3).values}\")\n",
    "        \n",
    "        # Final validation for 1h data (3 years)\n",
    "        if len(gld) < 8000:\n",
    "            raise Exception(f\"After cleaning, insufficient GLD data: {len(gld)} rows. Need at least 8000.\")\n",
    "        \n",
    "        # Save to cache using numpy arrays for better reliability\n",
    "        try:\n",
    "            print(f\"Saving to numpy cache: {cache_file}\")\n",
    "            \n",
    "            # Convert to numpy arrays with proper datetime handling for 1h data\n",
    "            data_array = gld[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
    "            dates_array = gld.index.strftime('%Y-%m-%d %H:%M:%S').values  # Include time for 1h data\n",
    "            \n",
    "            # Save arrays\n",
    "            np.save(cache_file, data_array)\n",
    "            np.save(cache_dates_file, dates_array)\n",
    "            \n",
    "            cache_size = os.path.getsize(cache_file) + os.path.getsize(cache_dates_file)\n",
    "            print(f\"✓ GLD Data cached as numpy arrays! ({len(gld)} rows, {cache_size} bytes)\")\n",
    "            \n",
    "            # Verify cache was written correctly\n",
    "            print(\"Verifying numpy cache integrity...\")\n",
    "            test_data = np.load(cache_file)\n",
    "            test_dates = np.load(cache_dates_file, allow_pickle=True)  # Fix: allow pickle for string arrays\n",
    "            \n",
    "            if len(test_data) != len(gld) or len(test_dates) != len(gld):\n",
    "                os.remove(cache_file)\n",
    "                os.remove(cache_dates_file)\n",
    "                print(\"✗ Cache verification failed, removed cache files\")\n",
    "            else:\n",
    "                print(\"✓ Numpy cache verification successful\")\n",
    "        except Exception as cache_error:\n",
    "            print(f\"✗ Warning: Could not cache GLD data: {cache_error}\")\n",
    "            # Continue without caching - not critical for training\n",
    "            for f in [cache_file, cache_dates_file]:\n",
    "                if os.path.exists(f):\n",
    "                    os.remove(f)\n",
    "        \n",
    "        print(\"=== GLD DATA FETCH COMPLETE ===\\n\")\n",
    "        return gld\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"GLD data fetch failed: {e}. Checking for cached data...\")\n",
    "        \n",
    "        # Try to use old cached data as last resort\n",
    "        if os.path.exists(cache_file) and os.path.exists(cache_dates_file):\n",
    "            try:\n",
    "                print(\"Attempting to use older cached GLD data...\")\n",
    "                data_array = np.load(cache_file)\n",
    "                dates_array = np.load(cache_dates_file)\n",
    "                dates = pd.to_datetime(dates_array)\n",
    "                cached_data = pd.DataFrame(data_array, \n",
    "                                         columns=['Open', 'High', 'Low', 'Close', 'Volume'],\n",
    "                                         index=dates)\n",
    "                \n",
    "                if len(cached_data) > 100:\n",
    "                    print(f\"Using old GLD cache: {len(cached_data)} rows\")\n",
    "                    return cached_data\n",
    "                else:\n",
    "                    print(\"Old GLD cache is also invalid\")\n",
    "            except Exception as cache_error:\n",
    "                print(f\"GLD cache load failed: {cache_error}\")\n",
    "            \n",
    "            # Remove corrupted cache files\n",
    "            for f in [cache_file, cache_dates_file]:\n",
    "                if os.path.exists(f):\n",
    "                    os.remove(f)\n",
    "        \n",
    "        print(\"Using synthetic GLD data as fallback...\")\n",
    "        # Generate synthetic gold price data (3 years)\n",
    "        dates = pd.date_range(start=datetime.now() - timedelta(days=1095), \n",
    "                             end=datetime.now(), freq='H')  # Hourly frequency\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        prices = [180]  # Start around typical GLD price\n",
    "        volumes = []\n",
    "        \n",
    "        for i in range(len(dates) - 1):\n",
    "            # Simulate realistic gold price movements (lower volatility, market hours effect)\n",
    "            hour = dates[i].hour\n",
    "            if 9 <= hour <= 16:  # Market hours - more activity\n",
    "                change = np.random.normal(0, 1.5)  \n",
    "            else:  # After hours - less volatility\n",
    "                change = np.random.normal(0, 0.5)\n",
    "            \n",
    "            prices.append(max(prices[-1] + change, 50))  # Min price of $50\n",
    "            \n",
    "            # Volume varies by market hours\n",
    "            if 9 <= hour <= 16:\n",
    "                volumes.append(np.random.randint(8000000, 25000000))\n",
    "            else:\n",
    "                volumes.append(np.random.randint(1000000, 5000000))\n",
    "        \n",
    "        volumes.append(np.random.randint(5000000, 20000000))\n",
    "        \n",
    "        synthetic_data = pd.DataFrame({\n",
    "            'Open': prices,\n",
    "            'High': [p * (1 + abs(np.random.normal(0, 0.008))) for p in prices],  # Lower vol \n",
    "            'Low': [p * (1 - abs(np.random.normal(0, 0.008))) for p in prices],   # Lower vol\n",
    "            'Close': prices,\n",
    "            'Volume': volumes\n",
    "        }, index=dates)\n",
    "        \n",
    "        return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "053f6148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GLD DATA FETCHING DEBUG ===\n",
      "Cache files: gld_data_1h.npy, gld_dates_1h.npy\n",
      "Cache exists: False\n",
      "Fetching fresh data or cache validation failed...\n",
      "\n",
      "=== FETCHING FRESH GLD DATA ===\n",
      "Connecting to Yahoo Finance for GLD...\n",
      "Requesting 1-HOUR GLD data from 2022-09-18 to 2025-09-17 (3 years)...\n",
      "Calling yf.download() with 1h interval for GLD (optimal for gold trading)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['GLD']: YFPricesMissingError('possibly delisted; no price data found  (1h 2022-09-18 20:04:46.301210 -> 2025-09-17 20:04:46.301210) (Yahoo error = \"1h data not available for startTime=1663545886 and endTime=1758132288. The requested range must be within the last 730 days.\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed in 0.69 seconds\n",
      "Downloaded data type: <class 'pandas.core.frame.DataFrame'>\n",
      "Downloaded data empty: True\n",
      "error occured: Downloaded GLD data is empty\n"
     ]
    }
   ],
   "source": [
    "df = fetch_gld_data()\n",
    "df.to_csv('initial_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e247a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
